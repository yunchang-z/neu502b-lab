{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `comp-00`: Multivariate pattern analysis (MVPA)\n",
    "This demo introduces multivariate pattern analysis (MVPA) using one of the datasets that popularized the application of machine learning to fMRI. Specifically, we will use classification models (e.g. the support vector machine; SVM) to differentiate between spatially-distributed cortical response patterns corresponding to different object stimuli (e.g. faces, houses, etc). Rather than using a regression model to predict brain activity at each voxel from the stimulus/task (sometimes referred to as an \"encoding\" model), multivariate pattern classification models predict the stimulus or task from distributed patterns of brain activity (referred to as a \"decoding\" model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual object recognition dataset\n",
    "We'll demo multivariate pattern classification on the visual object recognition dataset from [Haxby et al., 2001](https://doi.org/10.1126/science.1063736). This study popularized machine learning in fMRI and spurred a debate about localized versus distributed representation in human brain activity. Participants were presented with images from 8 object categories (bottles, cats, chairs, faces, houses, scissors, scrambled images, and shoes) interspersed with periods of fixation (referred to as \"rest\" here). The TR in this study was 2.5 seconds. In a given run, a block of images from each of the 8 categories was presented one time. Each block was ~9 TRs long and contained multiple rapid presentations of images from a single category. A subject received 12 scanning runs. We'll focus on data from one subject for the purposes of this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Haxby 2001 data\n",
    "from nilearn import datasets\n",
    "\n",
    "# If you're on your local machine, change this path\n",
    "data_dir = '/Users/snastase/Work/neu502b-2023/nilearn-data'\n",
    "\n",
    "haxby_dataset = datasets.fetch_haxby(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to extract certain attributes of the dataset, namely: the stimulus labels and the run labels. We'll exclude the the fixation TRs (the \"rest\" labels) from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in session metadata as pandas DataFrame\n",
    "session = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n",
    "\n",
    "# Extract stimuli and run labels for this subject\n",
    "stimuli, runs = session['labels'].values, session['chunks'].values\n",
    "\n",
    "# Create a boolean array indexing TRs containing a stimulus (non-rest)\n",
    "task_trs = stimuli != 'rest'\n",
    "\n",
    "# Get list of unique stimulus categories (excluding rest)\n",
    "categories = [c for c in np.unique(stimuli) if c != 'rest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the functional data using `index_img` to select only the task TRs; i.e. excluding intervening fixation (`'rest'`) periods. Check and interpret the shapes of the `stimuli_task` and `runs_task` labels with respect to the functional data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have the same number of TRs for data and labels:\n",
    "from nilearn.image import index_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than applying classification analysis to the whole brain, we'll focus on a specifically on ventral temporal (VT) cortex due to it's role in visual object and category representation. Create a `NiftiMasker` object for ventral temporal (VT) cortex for use later; set `standardize=True` in the NiftiMasker to ensure that masked time series are z-scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the VT mask file and creater masker:\n",
    "from nilearn.maskers import NiftiMasker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation using cross-validation\n",
    "To evaluate the quality of our classification model, we'll use a procedure called cross-validation. In this procedure, we train the model on a subset our data, then test the trained model on a left-out subset of data. This results in a classification score per cross-validation fold. When running a within-subjects classification analysis, the most common approach is to use leave-one-run-out cross-validation. In the current example, the dataset has 12 independent scanning runs. In leave-one-run-out cross-validation, the model will be trained on each subset of 11 runs and tested on the left-out 12th run, resulting in 12 classification scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sklearn's LeaveOneGroupOut cross-validation\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "cv = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrow a function from sklearn to visualize the cross-validation folds\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class', 'group']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(X)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a couple variables for plotting\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "stimuli_map = {c: i for i, c in enumerate(categories)}\n",
    "stimuli_int = [stimuli_map[s] for s in stimuli_task]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plot_cv_indices(cv, masker_vt.fit_transform(func_task),\n",
    "                stimuli_int, runs_task, ax, 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model\n",
    "Next, we'll load in Nilearn's `Decoder`, which provides a shortcut interface to some common classification models. We'll start with a basic support vector classifier (`svc`; [Boser et al., 1992](https://doi.org/10.1145/130385.130401)). There are a variety of ways to evaluate classifier performance. We'll use classification accuracy because this is a standard in the field (although other performance metrics maybe better; e.g. `roc_au`). Intialize the decoder with the SVC classifier using our leave-one-run-out cross-validation scheme above, specifying the VT mask and `accuracy` to evaluate performance. Get the cross-validated accuracy scores for the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nilearn's Decoder for SVM with cross-validation\n",
    "from nilearn.decoding import Decoder\n",
    "\n",
    "# Initialize Decoder with SVC, leave-one-out CV, and VT mask:\n",
    "\n",
    "# Fitting the decoder on the data across all CV folds:\n",
    "\n",
    "# Get scores for each class and CV fold:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the accuracy scores for each stumulus category below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification accuracy for each class:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using Nilearn's convenient `Decoder` function, we can also use classifiers directly from scikit-learn. For example, here we'll recreate a similar model using scikit-learn's `LinearSVC`. First, initalize the classifier. Then, use `cross_val_score` to fit and evaluate the classifier; print the resulting accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in automated cross-validation and classifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import warnings\n",
    "\n",
    "# Suppress some warnings (e.g. SVM convergence) just to clean up output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize SVC:\n",
    "\n",
    "# Run SVC with CV on VT data using cross_val_score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect classifier performance in more detail by visualizing the confusion matrix. Common metrics for quantifying classifer performance (e.g. accuracy, precision, AUROC) are summarizations of different aspects of the confusion matrix. Use `cross_val_predict` to re-run the classifier and store the predictions, then use `confusion_matrix` and `ConfusionMatrixDisplay` to visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Re-fit model to explicitly get predicted labels:\n",
    "\n",
    "# Create confusion matrix from true and predicted labels:\n",
    "\n",
    "# Plot confusion matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How \"good\" is our classification accuracy? Classification accuracy should be evaluated with respect to \"chance\" accuracy; i.e. the expected classification accuracy if the stimulus labels were randomly asigned. In a dataset with balanced class frequencies (i.e. same number of samples per class), the chance accuracy is typically $1 / n$ where $n$ is the number of distinct classes. For the current dataset, the chance accuracy is $1/8 = .125$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomizing labels\n",
    "To reinforce our intuition about \"chance\" decoding accuracy, we can shuffle the stimulus labels and re-run the entire classification algorithm. Although too computationally demanding for this demo, we could repeat this randomization procedure many times (e.g. 1000 permutations) to construct a null distibution (under the null hypothesis that there is no systematic relationship between class labels and activity patterns). This would amount to a permutation test and allow us to derive a *p*-value corresponding to our classification score. Below, use `np.random.permutation` to shuffle the stimulus labels, then re-run the classifier with `cross_val_score` and interpret the resulting accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle labels prior to classification:\n",
    "shuffled_labels = np.random.permutation(stimuli_task)\n",
    "\n",
    "# Re-run classifier using cross_val_score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the weight vectors\n",
    "Similarly to regression, training a classifier model yields coefficients (or \"weights\") assigned to each feature in the model. Unlike the GLM, in the decoding framework, the features of the model are voxels; thus, a weight is assigned to each voxel indicating it's importance for successful classification. We can visualize these weights on the brain. Note, however, that these weight vectors are more difficult to interpret than activation maps ([Haufe et al., 2014](https://doi.org/10.1016/j.neuroimage.2013.10.067)). Let's go back to the Nilearn's fitted `Decoder` object from a previous cell. Extract the `coef_img_` for faces and for houses and visualize them using `plot_stat_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weight vector for the face class:\n",
    "from nilearn.plotting import plot_stat_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weight vector for the house class:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-level GLM prior to classification\n",
    "In the previous analyses, we have fed each time point corresponding to a given stimulus class into the classifier. However, for some experimental designs, it may be more appropriate to first perform a GLM, then supply the classifier with the beta weights (i.e. coefficients) from the GLM. This reduces the number of samples (i.e. response patterns) for the classifier, but may result in cleaner samples. Here, we'll use Nilearn's `FirstLevelModel` to run a GLM separately for each run, then provide the resulting contrast maps to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build first-level GLM for each run\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "tr = 2.5\n",
    "\n",
    "events = {}\n",
    "for run in np.unique(runs):\n",
    "    stimuli_run = stimuli[runs == run]\n",
    "    n_trs = len(stimuli_run)\n",
    "    onset = tr * np.arange(n_trs)\n",
    "    duration = np.full(n_trs, tr)\n",
    "    \n",
    "    events_all = pd.DataFrame(\n",
    "        {'onset': onset, 'trial_type': stimuli_run, 'duration': duration})\n",
    "    events[run] = events_all[events_all['trial_type'] != 'rest']\n",
    "\n",
    "glm = FirstLevelModel(t_r=tr, hrf_model='spm',\n",
    "                      mask_img=mask_vt,\n",
    "                      drift_model='cosine',\n",
    "                      high_pass=1/128,\n",
    "                      standardize=True,\n",
    "                      noise_model='ar1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and collect beta maps\n",
    "glm_maps = []\n",
    "glm_categories = []\n",
    "glm_runs = []\n",
    "\n",
    "for run in np.unique(runs):\n",
    "    func_run = index_img(func_file, runs == run)\n",
    "    glm.fit(func_run, events=events[run])\n",
    "    for category in categories:\n",
    "        glm_maps.append(glm.compute_contrast(category))\n",
    "        glm_categories.append(category)\n",
    "        glm_runs.append(run)\n",
    "    print(f\"Finished fitting GLM for run {run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the GLM report\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.reporting import make_glm_report\n",
    "\n",
    "func_mean = mean_img(func_file)\n",
    "make_glm_report(glm,\n",
    "                contrasts=categories,\n",
    "                bg_img=func_mean,\n",
    "                threshold=0,\n",
    "                height_control=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-run the `Decoder` with the `glm_maps` produced above as input. Make sure to specify the `glm_categories` as your target variable, and specify `glm_runs` as the `groups` variable for leave-one-out cross-validation. How does the model perform on these GLM betas relative to the raw time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the decoder on the GLM data across runs:\n",
    "\n",
    "# Get scores for each class and CV fold:\n",
    "\n",
    "# Print classification accuracy for each class:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization using grid search\n",
    "Most classification models have multiple hyperparameters that will affect performance; for example, in SVMs, the hyperparameter $C$ controls the width of the margin used when positioning the decision boundary. The best hyperparameter setting will vary from dataset to dataset. In order to chose the best hyperparameter(s) in an unbiased way, we can evaluate a variety of hyperarameter settings (referred to as a \"grid\") using cross-validation nested within each of our training samples. In our previous examples, this hyperparameter optimization was either performed under the hood, or the software's default hyperparameter was used. Here, we'll use a larger grid of settings for $C$ in hopes of improving classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of C hyperparameter settings\n",
    "param_grid = {'C': np.logspace(-8, 2, 11)}\n",
    "print(f\"C hyperparamater grid:\\n {param_grid['C']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit classification model with hyperparameter grid\n",
    "decoder = Decoder(estimator='svc', cv=cv,\n",
    "                  mask=mask_vt, scoring='accuracy',\n",
    "                  param_grid=param_grid,\n",
    "                  standardize=False)\n",
    "decoder.fit(glm_maps, glm_categories, groups=glm_runs)\n",
    "\n",
    "# Get scores for each class and CV fold\n",
    "scores = decoder.cv_scores_\n",
    "\n",
    "# Print classification accuracy for each class\n",
    "for category in categories:\n",
    "    print(f\"Mean classification accuracy for {category} \"\n",
    "          f\"stimuli: {np.mean(scores[category]):.3f}\")\n",
    "print(\"Overall mean classification accuracy: \"\n",
    "      f\"{np.mean([scores[c] for c in categories]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting the local vs. distributed debate\n",
    "Finally, we can incorporate the functional localizer masks for cortical areas that are maximally responsive to faces and houses. Haxby and colleagues demonstrated that even cortical areas that prefer e.g. faces encode information about houses and other object categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each ROI and run decoding model\n",
    "roi_masks = {'VT':'mask_vt', 'face':'mask_face', 'house':'mask_house'}\n",
    "\n",
    "roi_accuracies = {}\n",
    "for roi in roi_masks:\n",
    "    mask_file = haxby_dataset[roi_masks[roi]][0]\n",
    "    decoder = Decoder(estimator='svc', cv=cv,\n",
    "                      mask=mask_file, scoring='accuracy')\n",
    "    decoder.fit(func_task, stimuli_task, groups=runs_task)\n",
    "    roi_accuracies[roi] = decoder.cv_scores_\n",
    "    print(f\"Finished decoding analysis for {roi} cortex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies for all three ROIs\n",
    "import seaborn as sns\n",
    "\n",
    "roi_long = {'ROI': [], 'category': [], 'accuracy': []}\n",
    "for roi in roi_accuracies:\n",
    "    for category in roi_accuracies[roi]:\n",
    "        for acc in roi_accuracies[roi][category]:\n",
    "            roi_long['ROI'].append(roi)\n",
    "            roi_long['category'].append(category)\n",
    "            roi_long['accuracy'].append(acc)\n",
    "roi_long = pd.DataFrame(roi_long)\n",
    "\n",
    "sns.barplot(data=roi_long, x='category', y='accuracy', hue='ROI')\n",
    "plt.axhline(.125, linestyle='--', color='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "* Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. In *Proceedings of the Fifth Annual Workshop on Computational Learning Theory* (pp. 144-152). https://doi.org/10.1145/130385.130401\n",
    "\n",
    "* Haufe, S., Meinecke, F., Görgen, K., Dähne, S., Haynes, J. D., Blankertz, B., & Bießmann, F. (2014). On the interpretation of weight vectors of linear models in multivariate neuroimaging. *NeuroImage*, *87*, 96-110. https://doi.org/10.1016/j.neuroimage.2013.10.067\n",
    "\n",
    "* Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L., & Pietrini, P. (2001). Distributed and overlapping representations of faces and objects in ventral temporal cortex. *Science*, *293*(5539), 2425–2430. https://doi.org/10.1126/science.1063736"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
