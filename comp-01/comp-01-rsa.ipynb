{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `comp-01`: Representational similarity analysis (RSA)\n",
    "This lab introduces representational similarity analysis (RSA) using two landmark datasets. Similarly to multivariate pattern classification, RSA captures the relationships among spatially distributed response patterns for different stimuli/conditions. However, rather than discretizing samples into two or more classes using a classification model, RSA simply measures the continuous similarity (or dissimilarity) between response patterns. Computing pairwise dissimilarities among response patterns yields a representational dissimilarity matrix (RDM), which can be compared to RDMs derived from other subjects, brain areas, species, or computational models ([Kriegeskorte et al., 2008a](https://doi.org/10.3389/neuro.06.004.2008))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual object recognition dataset\n",
    "We'll start applying RSA with our familiar visual object recognition fMRI dataset from [Haxby et al., 2001](https://doi.org/10.1126/science.1063736). Recall that participants were presented with images from 8 object categories (bottles, cats, chairs, faces, houses, scissors, scrambled images, and shoes) interspersed with periods of fixation (referred to as \"rest\" here). The TR in this study was 2.5 seconds. In a given run, a block of images from each of the 8 categories was presented one time. Each block was ~9 TRs long and contained multiple rapid presentations of images from a single category. A subject received 12 scanning runs. We'll focus on data from one subject for the purposes of this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haxby 2001 dataset and VT ROI mask\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "data_dir = '/Users/snastase/Work/neu502b-2023/nilearn-data'\n",
    "\n",
    "haxby_dataset = datasets.fetch_haxby(data_dir=data_dir)\n",
    "func_file = haxby_dataset.func[0]\n",
    "\n",
    "# Load in mask for VT OI\n",
    "mask_vt = haxby_dataset['mask_vt'][0]\n",
    "masker_vt = NiftiMasker(mask_img=mask_vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in session metadata as pandas DataFrame\n",
    "session = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n",
    "\n",
    "# Extract stimuli and run labels for this subject\n",
    "stimuli, runs = session['labels'].values, session['chunks'].values\n",
    "\n",
    "# Get list of unique stimulus categories (excluding rest)\n",
    "categories = np.array([c for c in np.unique(stimuli) if c != 'rest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split functional image according to runs\n",
    "from nilearn.image import index_img\n",
    "\n",
    "func_runs = []\n",
    "for run in np.unique(runs):\n",
    "    func_runs.append(index_img(func_file, runs == run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for RSA, we need response patterns corresponding to each of the 8 object categories in this dataset. To do this, we'll run a first-level GLM across all runs. This will yield a 8 maps of regression coefficients capturing responses to each of the 8 stimulus categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build first-level GLM for each run\n",
    "from nilearn.glm.first_level import (make_first_level_design_matrix,\n",
    "                                     FirstLevelModel)\n",
    "\n",
    "# Set parameters for you design matrix\n",
    "tr = 2.5\n",
    "hrf_model = 'spm'\n",
    "drift_model = 'cosine'\n",
    "high_pass = 1/128\n",
    "\n",
    "# Build a design matrix for each run\n",
    "design_matrices = []\n",
    "for run in np.unique(runs):\n",
    "    stimuli_run = stimuli[runs == run]\n",
    "    n_trs = len(stimuli_run)\n",
    "    onsets = tr * np.arange(n_trs)\n",
    "    duration = np.full(n_trs, tr)\n",
    "    \n",
    "    events_all = pd.DataFrame(\n",
    "        {'onset': onsets, 'trial_type': stimuli_run, 'duration': duration})\n",
    "    events = events_all[events_all['trial_type'] != 'rest']\n",
    "    \n",
    "    design_matrix = make_first_level_design_matrix(\n",
    "        onsets,\n",
    "        events,\n",
    "        hrf_model=hrf_model,\n",
    "        drift_model=drift_model,\n",
    "        high_pass=high_pass)\n",
    "    \n",
    "    design_matrices.append(design_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit GLM for all runs\n",
    "glm = FirstLevelModel(t_r=tr,\n",
    "                      mask_img=mask_vt,\n",
    "                      standardize=True,\n",
    "                      noise_model='ar1')\n",
    "\n",
    "glm.fit(func_runs, design_matrices=design_matrices)\n",
    "\n",
    "# Collate contrast maps for VT\n",
    "glm_vt = []\n",
    "for category in categories:\n",
    "    glm_map = glm.compute_contrast(category)\n",
    "    glm_vt.append(masker_vt.fit_transform(glm_map))\n",
    "\n",
    "glm_vt = np.vstack(glm_vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a representational dissimilarity matrix (RDM)\n",
    "To construct a representational dissimilarity matrix (RDM), we'll use SciPy's `pdist` to compute the pairwise distances between response patterns for each of the stimulus categories. We'll use *correlation distance* (i.e. $1 - r$) as our measure of dissimilarity according to the convention in the literature (note that Pearson correlation effectively normalizes the response patterns and has certain biases). The resulting RDM captures the *representational geometry* for the given stimulus set in our VT ROI. Since our RDM is symmetrical and the diagonal isn't very meaningful (representing a response pattern coorrelated with itself), we can more succinctly represent the RDM by vectorizing the cells in the off-diagonal triangle using the `squareform` function. The number of unique pairwise distances in the upper triangle of a $N \\times N$ matrix is $N * (N - 1) / 2$; i.e. for our $8 \\times 8$ matrix, there are $8 * (8 - 1) \\mathbin{/} 2 = 28$ values in the off-diagonal triangle. Below, use `pdist` to compute the (vectorized off-diagonal triangle) RDM for VT cortex, then use `squareform` and `plt.matshow` (or `sns.heatmap`) to visualize the square RDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RDM with correlation distance:\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "# Plot squareformed RDM:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the stimulus labels are alphabetically ordered. Let's try ordering them a bit more sensibly. Replot the RDM with reordered rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder RDM to reflect intuitive groups\n",
    "reorder = [3, 1, 0, 7, 5, 2, 4, 6]\n",
    "print(f\"Reordered categories: {categories[reorder]}\")\n",
    "\n",
    "# Plot squareformed RDM with reordered rows/columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also convert the correlation distance back to a correlation, resulting in an *similarity* matrix rather than a *dissimilarity* matix. Plot the correlation matrix corresponding to the RDM below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert correlation distance to correlation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other distance metrics will yield different RDMs. For example, unlike correlation distance, Euclidean distance is sensitive to regional-average response magnitudes (i.e. the difference in overall activation levels across an ROI). Recompute and plot an RDM using Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot RDM using Euclidean distance instead:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing representational geometries\n",
    "Our current *neural* RDM captures the representational geometry of VT cortex (for the current stimulus set). We can compare this RDM to *target* (or *model*) RDMs constructed to capture particular properties of the stimuli. For example, we'll manually construct an *animacy RDM* that captures the categorical relationship between human faces and cats—i.e. that they're both animate organisms, unlike scissors and chairs. We can also construct a *tool RDM* reflecting that scissors, bottles, and (arguably) shoes are all handheld, manipulable objects. In addition to simple categorical RDMs, we can also construct RDMs based on continuous features of the stimuli, like *real-world size*. We'll compare these target RDMs to the neural RDM using Spearman correlation. The \"trick\" of RSA is that no matter how we constructed these RDMs—whether they're based on computational models, behavioral judgments, response times, etc—we can always compare the second-order similarity structure captured by the RDMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up simple animacy RDM\n",
    "rdm_animacy = np.ones(len(rdm_vt))\n",
    "rdm_animacy[8] = .5\n",
    "\n",
    "# Set up simple tool RDM\n",
    "rdm_tools = np.ones(len(rdm_vt))\n",
    "rdm_tools[[4, 6, 26]] = .5\n",
    "\n",
    "# Set up real-world size RDM\n",
    "from itertools import combinations\n",
    "\n",
    "sizes = [.3, .35, .2, .2, .1, .5, 1, 0]\n",
    "rdm_size = []\n",
    "for pair in combinations(sizes, 2):\n",
    "    rdm_size.append(np.abs(pair[0] - pair[1]))\n",
    "\n",
    "# Plot all three RDMs\n",
    "rdms = {'animacy RDM': rdm_animacy,\n",
    "        'tool RDM': rdm_tools,\n",
    "        'real-world size RDM': rdm_size}\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "for rdm, ax in zip(rdms, axs):\n",
    "    sns.heatmap(squareform(rdms[rdm])[reorder][:, reorder],\n",
    "                ax=ax, square=True,\n",
    "                xticklabels=categories[reorder],\n",
    "                yticklabels=categories[reorder],\n",
    "                cbar_kws={'label': 'dissimilarity'})\n",
    "    ax.set_title(rdm)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Spearman correlations between these \"model\" RDMs and the neural RDM for VT cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations with VT RDM:\n",
    "from scipy.stats import spearmanr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering and dendrograms\n",
    "It's not always easy to see the structure when looking directly at an RDM. One way to visualize an RDM is to cluster the response patterns based on their similarity. Here, we use agglomerative hierarchical clustering to cluster the RDMs, then visualize this cluster hierarchy using a dendrogram. Use `linkage` from `scipy.cluster.hierarchy` to compute the clusters and use `dendrogram` to visualize them. Try a couple different linkage functions (e.g. `'single'`, `'complete'`, `'ward'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hierarchical clustering and plot dendrogram:\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional scaling (MDS)\n",
    "Another common and intuitive way to visualize the structure of an RDM is to use multidimensional scaling (MDS). MDS finds a low-dimensional space (i.e. 2-dimensional for visualization) that best preserves the pairwise distances between response patterns. Use `MDS` from `sklearn.manifold` to estimate a 2-dimensional projection from the precomputed dissimilarity values, and plot the resulting positions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit MDS to get positions in 2D space:\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "\n",
    "# Plot positions returned by MDS:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kriegeskorte dataset\n",
    "The previous dataset only contains 8 stimulus categories, and therefore does not provide a very rich description of the representational geometry in VT cortex. The dataset that popularized RSA contains 96 object images from a variety of categories, including human faces and body parts, nonhuman animal faces and bodies, artificial (i.e. human-made) inanimate objects, and natural inanimate objects [Kriegeskorte et al., 2008b](https://doi.org/10.1016/j.neuron.2008.10.043). In the following, we'll load in response patterns for FFA and PPA and revisit some of the previous analyses with this richer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Kriegeskorte fMRI data\n",
    "kriegeskorte_dataset = np.load('kriegeskorte_dataset.npz',\n",
    "                               allow_pickle=True)\n",
    "\n",
    "roi_data = kriegeskorte_dataset['roi_data'].item()\n",
    "category_names = kriegeskorte_dataset['category_names']\n",
    "category_labels = kriegeskorte_dataset['category_labels']\n",
    "images = kriegeskorte_dataset['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 96 stimulus images with category labels\n",
    "print(\"Category labels\")\n",
    "[print(f'{label}: {name}') for label, name in enumerate(category_names)]\n",
    "fig, axs = plt.subplots(6, 16, figsize=(16, 6))\n",
    "for image, label, ax in zip(images, category_labels, axs.flatten()):\n",
    "    ax.imshow(image);\n",
    "    ax.annotate(label, (.04, .77), color='white',\n",
    "                xycoords='axes fraction')\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, combine (i.e. column-stack) all provided ROIs into a single VT ROI for subject `'TI'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine left/right FFA and PPA into single VT ROI:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `pdist` to compute the RDM based on correlation distance. Optionally, you may want to z-score each voxel across samples prior to computing the pairwise distances. Plot the resulting RDM. A simple `rank_percentile` function is provided to support visualizations similar to the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple rank percentile function\n",
    "from scipy.stats import rankdata, zscore\n",
    "\n",
    "def rank_percentile(a):\n",
    "    return rankdata(a) / len(a) * 100\n",
    "\n",
    "# Create neural RDM for VT and plot:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use MDS to visualize this more complex RDM. Color the resulting samples according to the six categories supplied with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit MDS to get positions in 2D space:\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "\n",
    "# Plot positions returned by MDS:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "* Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L., & Pietrini, P. (2001). Distributed and overlapping representations of faces and objects in ventral temporal cortex. *Science*, *293*(5539), 2425–2430. https://doi.org/10.1126/science.1063736\n",
    "\n",
    "* Kriegeskorte, N., Mur, M., & Bandettini, P. A. (2008a). Representational similarity analysis—connecting the branches of systems neuroscience. *Frontiers in Systems Neuroscience*, *2*, 4. https://doi.org/10.3389/neuro.06.004.2008\n",
    "\n",
    "* Kriegeskorte, N., Mur, M., Ruff, D. A., Kiani, R., Bodurka, J., Esteky, H., Tanaka, K., & Bandettini, P. A. (2008b). Matching categorical object representations in inferior temporal cortex of man and monkey. *Neuron*, *60*(6), 1126–1141. https://doi.org/10.1016/j.neuron.2008.10.043"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
